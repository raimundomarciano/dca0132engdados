### PROTOCOLO

#Ã‰ importante iniciar o mongodb

```
docker exec -it mongodb mongosh
rs.initiate()

docker exec -it kafka kafka-topics --create --topic mongo.newdatabase.newcollection --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1

docker exec -it postgres psql -U user -d default_db
CREATE TABLE loan_info (loan DOUBLE PRECISION, "default" VARCHAR(20) );

$headers = @{"Content-Type"="application/json"}
Invoke-WebRequest -Uri "http://localhost:8083/connectors" -Method Post -Headers $headers -Body (Get-Content -Raw -Path "debezium-mongodb-connector.json")
```


# pega o IP do container debezium
docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' debezium

# registra o conector
$headers = @{"Content-Type"="application/json"}
Invoke-WebRequest -Uri "http://localhost:8083/connectors" -Method Post -Headers $headers -Body (Get-Content -Raw -Path "debezium-mongodb-connector.json")
Invoke-WebRequest -Uri "http://localhost:8083/connectors/mongodb-connector" -Method Delete -Headers $headers



docker cp sparkekafka.py spark-master:/opt/bitnami/spark/work/
docker cp depends/svm_credit.pkl spark-master:/opt/bitnami/spark/work/
docker cp depends/scaler.pkl spark-master:/opt/bitnami/spark/work/
docker cp depends/credito_teste.csv spark-master:/opt/bitnami/spark/work/
docker exec -it spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/bitnami/spark/work/sparkekafka.py

docker exec -it spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 --jars /opt/bitnami/spark/jars/mongo-spark-connector_2.12-10.1.1.jar /opt/bitnami/spark/work/sparkekafka.py

docker exec -it kafka kafka-topics --create --topic mongo.newdatabase.newcollection --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1
docker exec -it kafka kafka-topics --describe --topic mongo.newdatabase.newcollection --bootstrap-server kafka:9092
docker restart kafka


docker exec -it kafka kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic test-topic
docker exec -it kafka kafka-console-producer --broker-list kafka:9092 --topic test-topic

docker exec -it kafka kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic mongo.newdatabase.newcollection


docker start mongodb
docker exec -it mongodb mongosh

docker exec -it spark-master /bin/bash
docker stop spark-master
docker rm spark-master
docker-compose up --build spark-master


docker exec -it postgres psql -U user -d default_db
SELECT * FROM loan_info;



spark
localhost:8080

debezium
http://localhost:8083/connectors


docker-compose up --build
docker-compose down -v
docker exec -it mongodb mongosh

python3 -m venv mongo
source mongo/bin/activate
python3 -m pip install pyspark


#########

docker run -p 27017:27017 --name nosql-mongo -v mongo-data:/data/ -d mongo

https://dadosabertos.bndes.gov.br/ne/dataset/bacen/resource/caeb45c9-830c-455c-a1ef-becc535f934a

docker exec -it nosql-mongo /bin/bash
mongoimport --db bndes --collection contas --drop --file /var/tmp/lr-2021-3.json
docker exec -it nosql-mongo mongosh
use bndes
db.contas.countDocuments();
db.contas.find();

curl -o /tmp/mongo-kafka-source-connector.jar https://search.maven.org/remotecontent?filepath=org/mongodb/kafka/mongo-kafka-source/1.9.2/mongo-kafka-source-1.9.2.jar



